{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zohir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/zohir/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load messages dataset from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disasters_messages']\n"
     ]
    }
   ],
   "source": [
    "# Look for the table name\n",
    "\n",
    "db_file_name = 'DisasterResponse.db'\n",
    "engine = create_engine('sqlite:///'+db_file_name)\n",
    "print (engine.table_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape =  (26161, 38)\n"
     ]
    }
   ],
   "source": [
    "# load data from database\n",
    "db_file_name = 'DisasterResponse.db'\n",
    "db_table_name = 'disasters_messages'\n",
    "\n",
    "df = pd.read_sql_table(db_table_name, 'sqlite:///'+db_file_name)\n",
    "print('df.shape = ', df.shape)\n",
    "\n",
    "X = df['text'] \n",
    "categories_list = [c for c in df.columns if c not in ['genre', 'text']]\n",
    "categories_list.sort()\n",
    "y =  df[categories_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    return clean_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare data\n",
    "Split data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "Train a pipeline with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('multi_clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(ground_truth, predictions):\n",
    "\n",
    "    all_scores = []\n",
    "    for i, col in enumerate(categories_list):\n",
    "        cls_rep = classification_report(ground_truth[col], predictions[:,i])\n",
    "\n",
    "        # installed version 0.19 of classification_report does not allow easy dict export\n",
    "        # TODO: Update the requirement on the Udacity server to sklearn 0.20+\n",
    "        avg_scores = (cls_rep.split('\\n')[-2]).split()[-4:]\n",
    "        cur_scores = {'col':col}\n",
    "        for i, f in enumerate(['precision', 'recall','f1-score','support']):\n",
    "            cur_scores[f] = float(avg_scores[i]) \n",
    "        \n",
    "        all_scores.append(cur_scores)\n",
    "\n",
    "    all_scores_df = pd.DataFrame(all_scores).set_index('col')\n",
    "    return all_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_scores_df.mean =  0.926944444444\n",
      "(36, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aid_centers</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>6541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_related</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buildings</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>6541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>child_alone</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clothing</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>6541.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1-score  precision  recall  support\n",
       "col                                              \n",
       "aid_centers      0.98       0.98    0.99   6541.0\n",
       "aid_related      0.73       0.74    0.74   6541.0\n",
       "buildings        0.94       0.95    0.96   6541.0\n",
       "child_alone      1.00       1.00    1.00   6541.0\n",
       "clothing         0.98       0.99    0.99   6541.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the first model results and give a general score based on f1-scores of all categories\n",
    "all_scores_df = evaluate_results(y_validation, y_pred)\n",
    "print('all_scores_df.mean = ', all_scores_df['f1-score'].mean())\n",
    "print(all_scores_df.shape)\n",
    "all_scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom functon to score models by the gridSearch \n",
    "\n",
    "def custom_score_func(ground_truth, predictions):\n",
    "    all_scores_df = evaluate_results(ground_truth, predictions)\n",
    "    mean_f1_scores = all_scores_df['f1-score'].mean()\n",
    "    return mean_f1_scores\n",
    "\n",
    "# Get the customer scorer result for the first model\n",
    "custom_scorer = make_scorer(custom_score_func, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None ..\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None ..\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000 .\n",
      "[CV] tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000 .\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None ..\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None ..\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000 .\n",
      "[CV] tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000 .\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, score=0.9266666666666666, total=  23.4s\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, score=0.9252777777777776, total=  23.4s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None .\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, score=0.9305555555555556, total=  23.4s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None .\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   3 out of  16 | elapsed:   30.7s remaining:  2.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=10000, score=0.928611111111111, total=  23.6s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000 \n",
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, score=0.9244444444444445, total=  25.7s\n",
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, score=0.9244444444444443, total=  25.7s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None .\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   5 out of  16 | elapsed:   33.4s remaining:  1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, score=0.9272222222222223, total=  25.9s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   7 out of  16 | elapsed:   33.7s remaining:   43.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, score=0.9274999999999998, total=  26.2s\n",
      "[CV] tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000 \n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, score=0.9291666666666667, total=  22.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   9 out of  16 | elapsed:  1.0min remaining:   46.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=10000, score=0.9261111111111112, total=  22.7s\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, score=0.928611111111111, total=  22.6s\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=10000, score=0.9266666666666666, total=  22.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  11 out of  16 | elapsed:  1.1min remaining:   28.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, score=0.924722222222222, total=  25.5s\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=0.5, vect__max_features=None, score=0.9275, total=  25.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  13 out of  16 | elapsed:  1.1min remaining:   14.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, score=0.9244444444444445, total=  25.2s\n",
      "[CV]  tfidf__use_idf=False, vect__max_df=1.0, vect__max_features=None, score=0.9263888888888889, total=  25.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  16 out of  16 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=8,\n",
       "       param_grid={'vect__max_df': (0.5, 1.0), 'vect__max_features': (None, 10000), 'tfidf__use_idf': (True, False)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(custom_score_func), verbose=10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Improve the model by grid search parameters of the vect and tfidf transfomers\n",
    "\n",
    "parameters = {\n",
    "        'vect__max_df': (0.5, 1.0),\n",
    "        'vect__max_features': (None, 10000),\n",
    "        'tfidf__use_idf': (True, False)\n",
    "        }                                                        \n",
    "\n",
    "pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('multi_clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "        ])\n",
    "\n",
    "cv = GridSearchCV(estimator=pipeline, param_grid=parameters,\n",
    "                 verbose=10, n_jobs=8, cv=2, scoring=custom_scorer)\n",
    "\n",
    "\n",
    "cv.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_tfidf__use_idf</th>\n",
       "      <th>param_vect__max_df</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.276728</td>\n",
       "      <td>0.037835</td>\n",
       "      <td>7.128543</td>\n",
       "      <td>0.016389</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'tfidf__use_idf': True, 'vect__max_df': 0.5, ...</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.928611</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992778</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.000278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.507567</td>\n",
       "      <td>0.049440</td>\n",
       "      <td>7.067375</td>\n",
       "      <td>0.058202</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'tfidf__use_idf': False, 'vect__max_df': 0.5,...</td>\n",
       "      <td>0.929167</td>\n",
       "      <td>0.926111</td>\n",
       "      <td>0.927639</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>2</td>\n",
       "      <td>0.992778</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.992917</td>\n",
       "      <td>0.000139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15.648455</td>\n",
       "      <td>0.116127</td>\n",
       "      <td>6.871203</td>\n",
       "      <td>0.008938</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'tfidf__use_idf': False, 'vect__max_df': 1.0,...</td>\n",
       "      <td>0.928611</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.927639</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>2</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.992778</td>\n",
       "      <td>0.992917</td>\n",
       "      <td>0.000139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "1      16.276728      0.037835         7.128543        0.016389   \n",
       "5      15.507567      0.049440         7.067375        0.058202   \n",
       "7      15.648455      0.116127         6.871203        0.008938   \n",
       "\n",
       "  param_tfidf__use_idf param_vect__max_df param_vect__max_features  \\\n",
       "1                 True                0.5                    10000   \n",
       "5                False                0.5                    10000   \n",
       "7                False                  1                    10000   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "1  {'tfidf__use_idf': True, 'vect__max_df': 0.5, ...           0.930556   \n",
       "5  {'tfidf__use_idf': False, 'vect__max_df': 0.5,...           0.929167   \n",
       "7  {'tfidf__use_idf': False, 'vect__max_df': 1.0,...           0.928611   \n",
       "\n",
       "   split1_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "1           0.926667         0.928611        0.001944                1   \n",
       "5           0.926111         0.927639        0.001528                2   \n",
       "7           0.926667         0.927639        0.000972                2   \n",
       "\n",
       "   split0_train_score  split1_train_score  mean_train_score  std_train_score  \n",
       "1            0.992778            0.993333          0.993056         0.000278  \n",
       "5            0.992778            0.993056          0.992917         0.000139  \n",
       "7            0.993056            0.992778          0.992917         0.000139  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a unique general score of the best model found and a table of the grid search jobs\n",
    "\n",
    "a = pd.DataFrame(cv.cv_results_)\n",
    "a.sort_values(by='rank_test_score').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': 10000}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params of the best model found by this grid search\n",
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92861111111111105"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score of the best model found, on the data from the training dataset \n",
    "\n",
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': 10000}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_f1_scores =  0.932222222222\n",
      "(36, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aid_centers</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>6541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_related</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "      <td>6541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buildings</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>6541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>child_alone</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clothing</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>6541.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1-score  precision  recall  support\n",
       "col                                              \n",
       "aid_centers      0.98       0.98    0.99   6541.0\n",
       "aid_related      0.72       0.73    0.73   6541.0\n",
       "buildings        0.94       0.95    0.96   6541.0\n",
       "child_alone      1.00       1.00    1.00   6541.0\n",
       "clothing         0.98       0.98    0.99   6541.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results of the best model found by grid, applied to the validation dataset \n",
    "\n",
    "y_pred = cv.predict(X_validation)\n",
    "all_scores_df = evaluate_results(y_validation, y_pred)\n",
    "mean_f1_scores = all_scores_df['f1-score'].mean()\n",
    "print('mean_f1_scores = ', mean_f1_scores)\n",
    "print(all_scores_df.shape)\n",
    "all_scores_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "[CV] multi_clf__estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) \n",
      "[CV] multi_clf__estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) \n",
      "[CV] multi_clf__estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform') \n",
      "[CV] multi_clf__estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform') \n",
      "[CV]  multi_clf__estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), score=0.9063888888888888, total=   5.1s\n",
      "[CV]  multi_clf__estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), score=0.9094444444444443, total=   5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    7.9s remaining:    7.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  multi_clf__estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), score=0.9125000000000001, total= 2.0min\n",
      "[CV]  multi_clf__estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'), score=0.9205555555555556, total= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  4.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=4,\n",
       "       param_grid={'multi_clf__estimator': [MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(custom_score_func), verbose=10)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "parameters2 = {\n",
    "        'multi_clf__estimator': ([MultinomialNB(), KNeighborsClassifier()])\n",
    "            }                                                        \n",
    "\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('multi_clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    \n",
    "cv2 = GridSearchCV(estimator=pipeline2, \n",
    "                   param_grid=parameters2,\n",
    "                   verbose=10, n_jobs=4, cv=2, \n",
    "                   scoring=custom_scorer)\n",
    "\n",
    "\n",
    "cv2.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9408333333333333"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.score(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_file_name = '../models/cv2.model.save'\n",
    "pickle.dump(cv2, open(model_file_name, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_file_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-2c387f4b1d74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_file_name' is not defined"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(model_file_name, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
